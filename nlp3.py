# -*- coding: utf-8 -*-
"""NLP3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cAbi9Jas25QAEvmE5hwHrruu7z42qJar
"""

import pandas as pd
from bs4 import BeautifulSoup
import re, nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
print("done")

questions = pd.read_csv(
    "Questions.csv",
    encoding='latin-1',
    on_bad_lines='warn',  # or 'skip' to ignore bad rows
    engine='python'        # Use Python parser (slower but more tolerant)
)
ans = pd.read_csv(
    "Answers.csv",
    encoding='latin-1',
    on_bad_lines='warn',  # or 'skip' to ignore bad rows
    engine='python'        # Use Python parser (slower but more tolerant)
)
tags = pd.read_csv(
    "Tags.csv",
    encoding='latin-1',
    on_bad_lines='warn',  # or 'skip' to ignore bad rows
    engine='python'        # Use Python parser (slower but more tolerant)
)
print("done")

questions['ClosedDate'] = questions['ClosedDate'].fillna('Not Closed')
questions = questions.drop_duplicates(keep='first')
print('done')

questions.head()

ans.dropna(subset=['Body'], inplace=True)
ans['Body'] = ans['Body'].apply(
    lambda x: BeautifulSoup(x, 'html.parser').get_text()
)

ans = ans[ans['Body'].str.strip().astype(bool)]
ans.drop(['OwnerUserId', 'CreationDate'], axis=1, inplace=True, errors='ignore')
ans = ans.drop_duplicates(keep='first')

ans.head()

tags = tags.drop_duplicates(keep='first')

tags.head()

questions.dropna(subset=['Title', 'Body'], inplace=True)
# Remove HTML tags from Body
questions['Body'] = questions['Body'].apply(
    lambda x: BeautifulSoup(x, 'html.parser').get_text()
)

questions['Title'] = questions['Title'].apply(
    lambda x: BeautifulSoup(x, 'html.parser').get_text()
)
print("done")

# Remove empty text rows
questions = questions[questions['Title'].str.strip().astype(bool)]
questions = questions[questions['Body'].str.strip().astype(bool)]


print("done")

# Drop unnecessary columns
questions.drop(['ClosedDate', 'OwnerUserId', 'CreationDate'], axis=1, inplace=True, errors='ignore')
questions

tags = tags.dropna(subset=['Tag'])          # Drop rows with NaN
tags['Tag'] = tags['Tag'].astype(str)      # Convert to string
tags = tags[tags['Tag'] != 'nan']

tags_grouped = tags.groupby('Id')['Tag'].agg(list).reset_index()

questions = questions.merge(tags_grouped, on='Id', how='left')
questions

nltk.download(['stopwords', 'wordnet', 'omw-1.4'])
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))


def clean_text(text, technical_terms=None, use_lemmatizer=False):
    """Enhanced text cleaner for technical content

    Args:
        text: Input string
        technical_terms: Set of terms to preserve (e.g., {'SQL', 'API'})
        use_lemmatizer: False for stemmer, True for lemmatizer
    """
    if not isinstance(text, str):
        return ''

    # Preserve technical patterns
    text = re.sub(r'([a-z])([A-Z])', r'\1 \2', text)  # camelCase--> camel Case
    text = re.sub(r'_', ' ', text)  # snake_case
    text = re.sub(r'([A-Z]{2,})', r' \1', text)  # CONSTANTS

    # Clean special characters but preserve numbers
    text = re.sub(r'[^a-z0-9\s]', '', text.lower())

    # Technical term preservation
    preserved_terms = technical_terms or set()
    tokens = []
    for word in text.split():
        # Skip short words and stopwords
        if len(word) < 2 or word in stop_words:
            continue

        # Preserve technical terms
        if word in preserved_terms:
            tokens.append(word)
            continue

        # Apply stemming/lemmatization
        if use_lemmatizer:
            processed = lemmatizer.lemmatize(word)
        else:
            processed = stemmer.stem(word)

        tokens.append(processed)

    return ' '.join(tokens) if tokens else 'no_text'

# Apply cleaning to text
questions['Clean_Title'] = questions['Title'].apply(clean_text)
questions['Clean_Body'] = questions['Body'].apply(clean_text)
questions = questions[questions['Clean_Title'] != 'no_text']
questions = questions[questions['Clean_Body'] != 'no_text']
ans['Body'] = ans['Body'].apply(clean_text)
ans = ans[ans['Body'] != 'no_text']

tf_idf = TfidfVectorizer(
    max_features=20000,
    ngram_range=(1, 4),
    stop_words='english',
    min_df=5,
    max_df=0.85,
    sublinear_tf=True
)
X_tfidf = tf_idf.fit_transform(questions['Clean_Title'])
X_tfidf = tf_idf.fit_transform(questions['Clean_Body'])


# =============================================
# Final Output
# =============================================
print("\nFinal Questions Shape:", questions.shape)
print("TF-IDF Matrix Shape:", X_tfidf.shape)
print("\nSample Processed Questions:")
print(questions[['Id', 'Clean_Title', 'Tag', 'Clean_Body']].head(2))

questions

questions = questions.drop(
    ['Title', 'Body'],
    axis=1,
    errors='ignore'
)
questions

answers_with_questions = ans.merge(
    questions,
    left_on='ParentId',
    right_on='Id',
    how='left',
    suffixes=('_Answer', '_Question')
)

